{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prostate-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fallen-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./Corona_NLP_train.csv\", \n",
    "                       encoding = \"ISO-8859-1\")\n",
    "test_df  = pd.read_csv(\"./Corona_NLP_test.csv\", \n",
    "                       encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-ideal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hawaiian-compatibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41152</th>\n",
       "      <td>44951</td>\n",
       "      <td>89903</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Airline pilots offering to stock supermarket s...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41153</th>\n",
       "      <td>44952</td>\n",
       "      <td>89904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Response to complaint not provided citing COVI...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154</th>\n",
       "      <td>44953</td>\n",
       "      <td>89905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>You know itÂs getting tough when @KameronWild...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41155</th>\n",
       "      <td>44954</td>\n",
       "      <td>89906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Is it wrong that the smell of hand sanitizer i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41156</th>\n",
       "      <td>44955</td>\n",
       "      <td>89907</td>\n",
       "      <td>i love you so much || he/him</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41157 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName                      Location     TweetAt  \\\n",
       "0          3799       48751                        London  16-03-2020   \n",
       "1          3800       48752                            UK  16-03-2020   \n",
       "2          3801       48753                     Vagabonds  16-03-2020   \n",
       "3          3802       48754                           NaN  16-03-2020   \n",
       "4          3803       48755                           NaN  16-03-2020   \n",
       "...         ...         ...                           ...         ...   \n",
       "41152     44951       89903  Wellington City, New Zealand  14-04-2020   \n",
       "41153     44952       89904                           NaN  14-04-2020   \n",
       "41154     44953       89905                           NaN  14-04-2020   \n",
       "41155     44954       89906                           NaN  14-04-2020   \n",
       "41156     44955       89907  i love you so much || he/him  14-04-2020   \n",
       "\n",
       "                                           OriginalTweet           Sentiment  \n",
       "0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1      advice Talk to your neighbours family to excha...            Positive  \n",
       "2      Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3      My food stock is not the only one which is emp...            Positive  \n",
       "4      Me, ready to go at supermarket during the #COV...  Extremely Negative  \n",
       "...                                                  ...                 ...  \n",
       "41152  Airline pilots offering to stock supermarket s...             Neutral  \n",
       "41153  Response to complaint not provided citing COVI...  Extremely Negative  \n",
       "41154  You know itÂs getting tough when @KameronWild...            Positive  \n",
       "41155  Is it wrong that the smell of hand sanitizer i...             Neutral  \n",
       "41156  @TartiiCat Well new/used Rift S are going for ...            Negative  \n",
       "\n",
       "[41157 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-harassment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "built-motivation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...\n",
       "1        advice Talk to your neighbours family to excha...\n",
       "2        Coronavirus Australia: Woolworths to give elde...\n",
       "3        My food stock is not the only one which is emp...\n",
       "4        Me, ready to go at supermarket during the #COV...\n",
       "                               ...                        \n",
       "41152    Airline pilots offering to stock supermarket s...\n",
       "41153    Response to complaint not provided citing COVI...\n",
       "41154    You know itÂs getting tough when @KameronWild...\n",
       "41155    Is it wrong that the smell of hand sanitizer i...\n",
       "41156    @TartiiCat Well new/used Rift S are going for ...\n",
       "Name: OriginalTweet, Length: 41157, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"OriginalTweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sapphire-majority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"OriginalTweet\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suitable-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aggressive-harbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@MeNyrbie @Phil_Gahan @Chrisitv url and url and url'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'https?://\\S+', 'url', train_df[\"OriginalTweet\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-climb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-institute",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "medium-movie",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n",
      "\n",
      "advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order\n",
      "\n",
      "Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P\n",
      "\n",
      "My food stock is not the only one which is empty...\r",
      "\r\n",
      "\r",
      "\r\n",
      "PLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \r",
      "\r\n",
      "Stay calm, stay safe.\r",
      "\r\n",
      "\r",
      "\r\n",
      "#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\n",
      "\n",
      "Me, ready to go at supermarket during the #COVID19 outbreak.\r",
      "\r\n",
      "\r",
      "\r\n",
      "Not because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\r",
      "\r\n",
      "\r",
      "\r\n",
      "#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\n",
      "\n",
      "As news of the regionÂs first confirmed COVID-19 case came out of Sullivan County last week, people flocked to area stores to purchase cleaning supplies, hand sanitizer, food, toilet paper and other goods, @Tim_Dodson reports https://t.co/cfXch7a2lU\n",
      "\n",
      "Cashier at grocery store was sharing his insights on #Covid_19 To prove his credibility he commented \"I'm in Civics class so I know what I'm talking about\". https://t.co/ieFDNeHgDO\n",
      "\n",
      "Was at the supermarket today. Didn't buy toilet paper. #Rebel\r",
      "\r\n",
      "\r",
      "\r\n",
      "#toiletpapercrisis #covid_19 https://t.co/eVXkQLIdAZ\n",
      "\n",
      "Due to COVID-19 our retail store and classroom in Atlanta will not be open for walk-in business or classes for the next two weeks, beginning Monday, March 16.  We will continue to process online and phone orders as normal! Thank you for your understanding! https://t.co/kw91zJ5O5i\n",
      "\n",
      "For corona prevention,we should stop to buy things with the cash and should use online payment methods because corona can spread through the notes. Also we should prefer online shopping from our home. It's time to fight against COVID 19?. #govindia #IndiaFightsCorona\n",
      "\n",
      "All month there hasn't been crowding in the supermarkets or restaurants, however reducing all the hours and closing the malls means everyone is now using the same entrance and dependent on a single supermarket. #manila #lockdown #covid2019 #Philippines https://t.co/HxWs9LAnF9\n",
      "\n",
      "Due to the Covid-19 situation, we have increased demand for all food products. \r",
      "\r\n",
      "\r",
      "\r\n",
      "The wait time may be longer for all online orders, particularly beef share and freezer packs. \r",
      "\r\n",
      "\r",
      "\r\n",
      "We thank you for your patience during this time.\n",
      "\n",
      "#horningsea is a caring community. LetÂs ALL look after the less capable in our village and ensure they stay healthy. Bringing shopping to their doors, help with online shopping and self isolation if you have symptoms or been exposed to somebody who has. https://t.co/lsGrXXhjhh\n",
      "\n",
      "Me: I don't need to stock up on food, I'll just have Amazon deliver whatever I need #CoronaVirus\r",
      "\r\n",
      "\r",
      "\r\n",
      "Amazon: https://t.co/8YWaKFjExC\n",
      "\n",
      "ADARA Releases COVID-19 Resource Center for Travel Brands: Insights Help Travel Brands Stay Up-To-Date on Consumer Travel Behavior Trends https://t.co/PnA797jDKV https://t.co/dQox6uSihz\n",
      "\n",
      "Lines at the grocery store have been unpredictable, but is eating out a safe alternative? \r",
      "\r\n",
      "\r",
      "\r\n",
      "Find out more about whether you should be avoiding restaurants right now:  https://t.co/9idZSis5oQ\r",
      "\r\n",
      "\r",
      "\r\n",
      "#coronavirus #covid19 https://t.co/ZHbh898lf6\n",
      "\n",
      "????? ????? ????? ????? ??\r",
      "\r\n",
      "?????? ????? ??????? ????????? ?\r",
      "\r\n",
      "#????_???? ????? ???? ?????? ? #????????? ?????? ?? 13 ???? ?? ?? ???? ?????? ?? ??\r",
      "\r\n",
      "#???_???????? ????? ??? ???? ? https://t.co/51bL8P6vZh\n",
      "\n",
      "@eyeonthearctic 16MAR20 Russia consumer surveillance watchdog reported case in high Arctic where a man who traveled to Iran has COVID-19 and 101 are \"observed\"\r",
      "\r\n",
      "https://t.co/4WnrrK9oKC https://t.co/ld05k5Eyns\n",
      "\n",
      "Amazon Glitch Stymies Whole Foods, Fresh Grocery Deliveries\r",
      "\r\n",
      "ÂAs COVID-19 has spread, weÂve seen a significant increase in people shopping online for groceries,Â a spokeswoman said in a statement. ÂToday this resulted in a systems impact affecting our ...\r",
      "\r\n",
      " https://t.co/TbzZ2MC3b3\n",
      "\n",
      "For those who aren't struggling, please consider donating to a food bank or a nonprofit. The demand for these services will increase as COVID-19 impacts jobs, and people's way of life.\n",
      "\n",
      "with 100  nations inficted with  covid  19  the world must  not  play fair with china  100 goverments must demand  china  adopts new guilde  lines on food safty  the  chinese  goverment  is guilty of  being  irosponcible   with life  on a global scale\n",
      "\n",
      "https://t.co/AVKrR9syff\r",
      "\r\n",
      "\r",
      "\r\n",
      "The COVID-19 coronavirus pandemic is impacting consumer shopping behavior, purchase decisions and retail sales, according to a First Insight study.\n",
      "\n",
      "We have AMAZING CHEAP DEALS! FOR THE #COVID2019 going on to help you???\r",
      "\r\n",
      "#Trials\r",
      "\r\n",
      "#Monthly\r",
      "\r\n",
      "#Yearly \r",
      "\r\n",
      "And Resonable #Prices / #Subscriptions\r",
      "\r\n",
      "Just DM US! #bestiptv #iptv #Service #Iptv #iptvdeals #Cheap #ipTV #Football #HD #Movies #Adult #Cinema #hotmovies #iptvnew #iptv2020 #Adult\n",
      "\n",
      "We have AMAZING CHEAP DEALS! FOR THE #COVID2019 going on to help you???\r",
      "\r\n",
      "#Trials\r",
      "\r\n",
      "#Monthly\r",
      "\r\n",
      "#Yearly \r",
      "\r\n",
      "And Resonable #Prices / #Subscriptions\r",
      "\r\n",
      "Just DM US! #bestiptv #iptv #Service #Iptv #iptvdeals #Cheap #ipTV #Football #HD #Movies #Adult #Cinema #hotmovies #ipTv IPTVLinks #18Movies\n",
      "\n",
      "@10DowningStreet @grantshapps what is being done to ensure food and other essential products are being re-stocked at supermarkets and panic buying actively discouraged? It cannot be left to checkout staff to police the actions of the selfish and profiteer\n",
      "\n",
      "UK #consumer poll indicates the majority expect #covid19's impact to last 4-12 months (at 12 March). We expect this to increase at the next #tracker... See full results of the @RetailX Coronavirus Consumer Confidence Tracker here: https://t.co/K3uJlcjqDB https://t.co/9G3kgqIXJ8\n",
      "\n",
      "In preparation for higher demand and a potential food shortage, The Hunger Coalition purchased 10 percent more food and implemented new protocols due to the COVID-19 coronavirus. https://t.co/5CecYtLnYn\n",
      "\n",
      "This morning I tested positive for Covid 19. I feel ok, I have no symptoms so far but have been isolated since I found out about my possible exposure to the virus.  Stay home people and be pragmatic. I will keep you updated on how IÂm doing ???? No panic. https://t.co/Lg7HVMZglZ\n",
      "\n",
      "Do you see malicious price increases in NYC? The NYC Department of Consumer and Worker Protection (DCWP) has set up a page to digitally file a complaint. Click here: https://t.co/oEx6Y8mm2K\r",
      "\r\n",
      "\r",
      "\r\n",
      "To file a complaint (use the word\"Overcharge\") https://t.co/MdMmoBttOP\r",
      "\r\n",
      "#COVID19 #CovidNYC\n",
      "\n",
      "@7SealsOfTheEnd Soon with dwindling supplies unlawful Panicky people will be breaking into Closed Stores &amp; Supermarkets to Raid them as they normally do during a Crisis so massive as the #Coronavirus\r",
      "\r\n",
      "\r",
      "\r\n",
      "#StockUp&amp;LockUp\n",
      "\n",
      "There Is of in the Country  The more empty shelves people see the more buying ensues the more food is out of stock\n",
      "\n",
      "'Hole' Foods...\r",
      "\r\n",
      "\r",
      "\r\n",
      "...images from the nicest grocery store in one of the richest neighborhoods in the United States.\r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/WnQSoMtkVI\r",
      "\r\n",
      "\r",
      "\r\n",
      "#BreakingNews #Breaking #Coronavirus #CoronavirusOutbreak #COVID19 #COVID?19 #COVID_19 #COVID2019 #Collapse\n",
      "\n",
      "Retail store closures could explode because of the #coronavirus (via @CNBC). #BrickAndMortar\r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/hQrYRNXFhv https://t.co/g5UZn06gb6\n",
      "\n",
      "Coronavirus fun fact: if you cough at the grocery store, you get the whole aisle to yourself pretty quickly.\r",
      "\r\n",
      "#CoronavirusOutbreak #coronavirus #COVID2019\n",
      "\n",
      "We're sorry to say that our @FinFabUK event is being cancelled due to Covid-19. The health and wellbeing of our attendees, speakers and staff is our top priority. Apologies for any disappointment this may cause. All FAQs are answered in the link below:\r",
      "\r\n",
      "https://t.co/GDDPTudCvj\n",
      "\n",
      "Went to the supermarket yesterday and the toilet paper was gone. Has this anything to do with the Corona virus? #COVID2019\n",
      "\n",
      "Yes, buy only what you need.\r",
      "\r\n",
      "\r",
      "\r\n",
      "But what's the point of posting photos of those people in the supermarket with a load of stuff? They could be buying for all their elderly parents, kids, siblings, etc who can't buy for themselves\r",
      "\r\n",
      "\r",
      "\r\n",
      "Not everything needs to be viral, Covid-19 alr is\n",
      "\n",
      "Worried about the impact of the current COVID-19 pandemic on your finances? WeÂve just published some tips to help you manage your money during these challenging times. #COVID19 https://t.co/3jKK3CqXfQ https://t.co/EbEnURmmJS\n",
      "\n",
      "my wife works retail&amp;a customer came in yesterday, coughing everywhere, saying they have CoVid-19. They requested a deep clean of the store - her company objected to due to cost, recommending the team spray disinfectant&amp;clean themselves. we're gonna die/get sick due to capitalism\n",
      "\n",
      "Now I can go to the supermarket like this without being judged! ? #CoronavirusOutbreak #COVID2019 https://t.co/krTCGiUHQS\n",
      "\n",
      "We're here to provide a safe shopping experience for our customers and a healthy environment for our associates and community!\r",
      "\r\n",
      "Online orders can be placed here: https://t.co/dCSXHUj3U0\r",
      "\r\n",
      "\r",
      "\r\n",
      "#jlmco #jlmcobrand #coronapocolypse #coronavirus #CoronavirusOutbreak  #COVID19 #shoponline https://t.co/riNKwskeRS\n",
      "\n",
      "Curious,  do we think retail shoppers will do a lot of online shopping bc they're home and unable to go out or do we think everyone is too spooked to get that extra pair of shoes? #economy #onlineshopping #coronavirus #covid19 #stayhome\n",
      "\n",
      "CHECK VIDEO ?? https://t.co/1ksn9Brl02 ??No food ? in USA market due to coronavirus panic we gonna die from starvation #CoronavirusOutbreak #coronavirus #houston #nofood #Notoiletpaper #NoHandShakes #nohandsanitizer #COVID19 #pandemic #totallockdown #COVID2019usa #walmart https://t.co/ztN3iMkgpD\n",
      "\n",
      "Breaking Story: Online clothes shopping rises as people find mysterious white patches forming on clothes. #QuarantineLife #CoronavirusOutbreak #coronavirus #IMadeThisUp #FakeNews https://t.co/5Z24hptT9M\n",
      "\n",
      "This is the line outside  @Target  in as customers wait for the store to open this morning\n",
      "\n",
      "South Africans stock up on food, basic goods as coronavirus panic hits https://t.co/6nGNFJmy89\r",
      "\r\n",
      "\r",
      "\r\n",
      "#CoronaVirusSA \r",
      "\r\n",
      "#Covid_19 https://t.co/pzirO10avf\n",
      "\n",
      " Please Share  Know someone who s 65 Living on their own struggling to get 2 their local supermarket due to issues around 19 We re offering FREE deliveries of our healthy soups NATIONWIDE to anyone 65 in need Plus their freezable\n",
      "\n",
      "People posting and sharing photos of of half to completely empty shelves calling those people \"dumb\" or \"idiots.\" All while shopping at the grocery store. lol\r",
      "\r\n",
      "\r",
      "\r\n",
      "#coronavirus #COVID19\n",
      "\n",
      "Never thought I'd say this, but. 2019, Will you come back!? PLEASE! #coronavirus #COVID19 #peoplearelosingtheirminds #StopTheMadness #stoppanicbuying\n",
      "\n",
      "COVID-19 restrictions sparking a run on cannabis stores\r",
      "\r\n",
      "\r",
      "\r\n",
      "They're not closed yet! But Customers are stocking up on cannabis this weekend, preparing for what could be more retail store restrictions in coming days. https://t.co/WMqR8QWoiG\n",
      "\n",
      "\"Everything weÂre seeing in the current COVID-19 outbreak has been seen before in previous epidemics and pandemics; the rise of fear, racism, panic buying of food and medicines, conspiracy theories, the proliferation of quack cures\" https://t.co/Pr8NpKX41A\n",
      "\n",
      "Everyone is closed, but we remain open because we are an emergency store. Thank your retail workers.\r",
      "\r\n",
      "\r",
      "\r\n",
      "#covid_19 #pandemic #socialdistancing #retail https://t.co/WtB0B1AMON\n",
      "\n",
      "Why we stock up on water... cause utility companies will shut you off in the middle of a pandemic... the schools close thier doors, you lose out on work cause your kid has no where to go... and you canÂt afford months worth of food. #coronavirus @SenatorRomney https://t.co/0CV0793olS\n",
      "\n",
      "Dear Coronavirus, \r",
      "\r\n",
      "I've been following social distancing rules and staying home to prevent the spread of you.  However, now I've spent an alarming amount of money shopping online.  Where can I submit my expenses to for reimbursement? Let me know.\r",
      "\r\n",
      "#coronapocolypse #coronavirus\n",
      "\n",
      "Global food prices before the spread of COVID 19 intensified across several geographies We could see further downward pressures in the coming months due to continued well supplied markets and the negative impact on demand resulting from the virus\n",
      "\n",
      "Morning everyone have a great and safe day. ??? #coronavirus #StopPanicBuying #BeKind #mufc #MUFC_Family\n",
      "\n",
      "Of all the things to panic buy in an emergency, I don't get why toilet paper is so important. If you're afraid of the worst case scenario, just wash up in the tub and use your money on food. Y'all crazy. #coronavirus\n",
      "\n",
      "THANK YOUR GROCERY CLERK!\r",
      "\r\n",
      "Went to grocery store today and looked into the weary eyes of the clerk.\r",
      "\r\n",
      "I thanked her and realized that she was thrust on the front line of this panick. A new breed of first responders? They are working hard to serve their communities. #coronavirus\n",
      "\n",
      "With the outbreak of Covid-19 in entire world, the retail shops in Malaysia is facing a great challenges. In the near future, online shopping will be a surprise way for all the people while many will lost their jobs. #Malaysia2020 #Malaysia #COVID?19\n",
      "\n",
      "My thoughts on impacts of coronavirus on food markets\r",
      "\r\n",
      "https://t.co/bPodDdPRcE\n",
      "\n",
      "Consumer Corner: #Scammers Taking Advantage Of #COVID-19 Fears\r",
      "\r\n",
      "\r",
      "\r\n",
      "#coronavirus #cdc #flu #trends #alert\r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/sk9qCJsnYl https://t.co/T7qejP3hys\n",
      "\n",
      "4. \"Both the masks made for medical personnel and for consumer purchase require a once-obscure material called melt-blown fabric.\"\r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/3hCd9IiWoX\n",
      "\n",
      "My work is capitalizing on the  demand for packaged food and making us stay open as opposed to closing for all our health and safety #LockdownCanada\r",
      "\r\n",
      "#coronavirus\n",
      "\n",
      "So, are we feeling like it's ethical to still do stuff like order deliveries (food, online shopping, etc.), ship \"isolation\" care packages to loved ones, etc.? #COVID2019\n",
      "\n",
      "What 2K Consumers Told PYMNTS About How COVID-19 Changed Their Daily Lives https://t.co/Ybg8Zupdf6 via @pymnts\n",
      "\n",
      "Bought a house during Covid-19 panic. DidnÂt think to buy food for the house. Tragic.\n",
      "\n",
      "Seen in a Facebook group - businesses need to stop increasing prices on essentials while we are in an emergency situation - itÂs frankly despicable and is totally void community spirit! #nameandshame #covid?19uk #coronavirus #Liverpool https://t.co/StTAkyqQiZ\n",
      "\n",
      "@BobJLowe Sadly those are the misinformed thinking that COVID-19 gives diarrhoea, therefore they had to stock-pile toilet papers. ? ATM, hygiene and food are more important.\n",
      "\n",
      "@TinaMcCauley70 Yeah my parents are risky people to the covid 19 thatÂs why we stay at home just go to the supermarket when really necessary.. stay safe too ....\n",
      "\n",
      "CN - #Coronavirus #COVID19 \r",
      "\r\n",
      "\r",
      "\r\n",
      "I will be in the group (and so will my Mum, who I live with) in the group that needs to be \"shielded\" for 12 weeks (3 months). This will mean staying in. I hope I can still get the online shopping that I need.\n",
      "\n",
      "ItÂs kind of like how saying a word over and over makes it not sound like a word anymore.\r",
      "\r\n",
      "\r",
      "\r\n",
      "For many of the people who donÂt think the COVID-19 news is BS, itÂs making them go to the stores and panic buy food and basic necessities until thereÂs nothing left.\n",
      "\n",
      "Hi, COVID-19. Thanks for making me do more online shopping.\n",
      "\n",
      "Corona scare sends sea-food prices skyrocketing in Mumbai\r",
      "\r\n",
      "\r",
      "\r\n",
      "&gt;&gt; https://t.co/GB11EFBYIB \r",
      "\r\n",
      "\r",
      "\r\n",
      "#seafood #coronavirus #CoronavirusOutbreak #CoronavirusReachesDelhi #Coronavid19 #CoronaVirusUpdates #COVID2019 #COVID19 #JhalakBollywood #JhalakKollywood #JhalakTollywood https://t.co/U5Dg3LoFYG\n",
      "\n",
      "Pausing student loan payments in addition to halting interest accumulation amp stopping punitive student loan collections would provide much needed immediate relief to those individuals unable to work amp are facing economic hardship\n",
      "\n",
      "@balajis On the consumer side - the tech is there (some Chinese group already demostrated ELISA test strips for COVID-19, though details were lacking). For consumer though @US_FDA would have to deem it as a waived test, which doesnÂt come that easily\n",
      "\n",
      "Lost wages either due to illness from 19 or to the virus  economic impact will mean an increased demand We urge and to support a bill that includes support for food banks flexibility for and school meals and increased\n",
      "\n",
      "The actions of some are so selfish. If I were CEO of a grocery store, from 7-9 am would be a time for people over 65 to shop; show ID. I just saw a young couple with 300 rolls of tp. No one is that full of crap. Well maybe\r",
      "\r\n",
      "\r",
      "\r\n",
      "#CoronavirusOutbreak \r",
      "\r\n",
      "\r",
      "\r\n",
      " https://t.co/Hrbzmh95VQ\n",
      "\n",
      "Coronavirus poses a complex puzzle for food-delivery companies - their delivery capacity may buckle under surging demand. https://t.co/1C1cMLmQii via @WSJ #services #food #delivery #coronavirus\n",
      "\n",
      "@TheJoshuaTurner @Loreign83 @peanut_astro @my_amigouk @afneil @BorisJohnson @patel4witham This is both disgusting and disgraceful charging over inflated prices for items for stopping the spread of COVID-19, the government really needs to do something abou\n",
      "\n",
      "As more retailers close physical stores or curtail hours as a result of Covid-19, it is agoing to put additional pressure on other omnichannel alternatives like grocery delivery and curbside pick up.  https://t.co/kgyDow3Nrz #covid19 #ecommerce #omnichannel #retail #digital\n",
      "\n",
      "Check out what these folks are up to here in So Cal ? I like this idea ?\r",
      "\r\n",
      "\r",
      "\r\n",
      "La Habra supermarket offers special hours for seniors amid COVID-19 crisis https://t.co/ncTXF8TGyf\n",
      "\n",
      "Love it or hate it, head advice @10DowningStreet &amp; @BorisJohnson \r",
      "\r\n",
      "Blip in our lives but itÂs happening!\r",
      "\r\n",
      "?? DonÂt whinge about what you canÂt do \r",
      "\r\n",
      "?? Dont panic buy as food wont run out\r",
      "\r\n",
      "?? DO spend time with the family\r",
      "\r\n",
      "?? DO use common sense\r",
      "\r\n",
      "#coronavirus @WorldHealthOrg2\n",
      "\n",
      "An open letter to consumer-debt holding organizations and others: We are at the precipice of a crisis of household economy. Please suspend debts (and interest/fees) for sixty days in response to the COVID-19 crisis.\r",
      "\r\n",
      "\r",
      "\r\n",
      "Feel free to sign here:\r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/JMZZJOmNT9 https://t.co/YR2tcPx1Ee\n",
      "\n",
      "#COVID?19 #COVID19Aus #coronavirus Just wanted to spread this news to all older Australians, particularly those still mobile but without family support: https://t.co/yDGX4lK8L0\n",
      "\n",
      "Sadly, this does not surprise me\r",
      "\r\n",
      "Heard from one payer exec that they are laying low, hoping all blows over. \r",
      "\r\n",
      "\r",
      "\r\n",
      "Mind bogglingly stupid and this was a nonprofit Blues plan! https://t.co/zr67d1u12Q\n",
      "\n",
      "I work in retail I keep stock back for our older customers so when such as frank comes in store for his bread and he sees a empty shelve I say donÂt worry pal IÂve saved you 1. Same as pat n her beans. Could I get disciplined? Yes do I care ? No IÂve got a ?? #coronavirus\n",
      "\n",
      "In attempts to lengthen runways marketing budgets are being slashed hiring is being frozen and staffing matrices are being redrawn and dive deep into how consumer startups are battling the impact of on their business\n",
      "\n",
      "...Âat this time, our distillery remains in operation, but we will not be offering public tours or hosting functions or events. Our retail store is also closed...\"\r",
      "\r\n",
      "https://t.co/lYZg2kfsm0\n",
      "\n",
      "Please don't hoard food and water. There's absolutely no need to panic buy; the supply chain is completely interrupted. And above all, please don't hoard sanitizing products; there are people out there who really need them, probably more than you. #DontPanicBuy #coronavirus\n",
      "\n",
      "The fact that canned food, toxic chemicals and store bought hand sanitizers are out of stock, yet fresh fruit, vegetables and herbs are FULLY stocked, shows that humans have no idea how the immune system works.\r",
      "\r\n",
      "#QuarantineLife #COVID2019\n",
      "\n",
      "Just called mum and dad in UK (over 70). They are great but I offered help with online shopping etc. We might sometimes forget that this is not always easy.\r",
      "\r\n",
      "\r",
      "\r\n",
      "Do the same if you can. ??\r",
      "\r\n",
      "If you are far from your parents like me Tech can be really useful. #COVID?19  #Coronavirus\n",
      "\n",
      "People seen stocking up on goods into trolleys after the panic buying rumours spread today at hypermarket in Kajang March 16 2020 Picture by Shafwan Zaidon\n",
      "\n",
      "As we often see during major news events, criminals will try to take advantage of any situation. The Coronavirus, or COVID-19, is no exception. Here is some guidance from the Attorney General's office: https://t.co/KT5J4QqCwS\n",
      "\n",
      "Pretty sure within a week or two, supermarket supply chains will dry up as more counties are effected by Covid-19 (and possibly go into lockdown). If so, would the government introduce a form of rationing so that people can eat? Somehow I don't think so.\n",
      "\n",
      "Supermarket workers are at the frontline of COVID-19.\r",
      "\r\n",
      "\r",
      "\r\n",
      "These are extraordinary times and retail is under extreme pressure.\r",
      "\r\n",
      "\r",
      "\r\n",
      "When shopping, please remain calm and thank the workers that are doing everything they can to keep the shelves stocked and the checkouts moving. https://t.co/0uHGM8gsp8\n",
      "\n",
      "Worried about COVID-19? \r",
      "\r\n",
      "\r",
      "\r\n",
      "I more worried about people panicking.\r",
      "\r\n",
      "\r",
      "\r\n",
      "Having a plan:\r",
      "\r\n",
      "\r",
      "\r\n",
      "&gt; You're not panic buying food.\r",
      "\r\n",
      "&gt; You can focus on the important issues. \r",
      "\r\n",
      "&gt; You have the best opportunity for a positive outcome.\r",
      "\r\n",
      "\r",
      "\r\n",
      "Create structure, reduce key decisions. Flourish\r",
      "\r\n",
      "\r",
      "\r\n",
      "Stay safe.\n",
      "\n",
      ".@kroger is the biggest supermarket chain in the United States.\r",
      "\r\n",
      "\r",
      "\r\n",
      "It has 453,000 employees and many receive no sick leave.\r",
      "\r\n",
      "\r",
      "\r\n",
      "Even after 2 employees tested positive for COVID-19, @kroger still won't provide paid sick leave to everyone\r",
      "\r\n",
      "\r",
      "\r\n",
      "https://t.co/19uNybttHl\n",
      "\n",
      "@kroger Instead of paid sick leave, @kroger is providing 2 weeks paid leave ONLY to people who test positive for COVID-19 or are placed under mandatory quarantine\r",
      "\r\n",
      "\r",
      "\r\n",
      "This is insufficient to protect staff and the public, especially with little testing av\n",
      "\n",
      "I followed this when I went shopping a few days ago. It's a pain but necessary! Protect Yourself From Grocery Shopping - Consumer Reports #COVID2019 #StayHealthy https://t.co/48nG14me6E\n",
      "\n",
      "@joncoopertweets I took these pictures today at my home grocery store in Montgomery County, MD. No flour, sugar, sweet potatoes, potatoes, orange juice, paper towels, or toilet paper. Low on meat, mac &amp; cheese. #coronapocolypse #Covid_19 #panicbuying \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in train_df[\"OriginalTweet\"][:100]:\n",
    "    print(t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-remainder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "technical-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(ss):\n",
    "    ss = re.sub(r'https?://\\S+', 'url', ss)\n",
    "    ss = re.sub(r'#', ' # ', ss)\n",
    "    ss = re.sub(r'\\s+', ' ', ss)\n",
    "    \n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-saying",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "played-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"ProcessedTweet\"] = train_df[\"OriginalTweet\"].apply(clean_string)\n",
    "test_df[\"ProcessedTweet\"]  = test_df[\"OriginalTweet\"].apply(clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-science",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "national-initial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ProcessedTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv url and url an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>Me, ready to go at supermarket during the # CO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41152</th>\n",
       "      <td>44951</td>\n",
       "      <td>89903</td>\n",
       "      <td>Wellington City, New Zealand</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Airline pilots offering to stock supermarket s...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Airline pilots offering to stock supermarket s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41153</th>\n",
       "      <td>44952</td>\n",
       "      <td>89904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Response to complaint not provided citing COVI...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>Response to complaint not provided citing COVI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154</th>\n",
       "      <td>44953</td>\n",
       "      <td>89905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>You know itÂs getting tough when @KameronWild...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>You know itÂs getting tough when @KameronWild...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41155</th>\n",
       "      <td>44954</td>\n",
       "      <td>89906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Is it wrong that the smell of hand sanitizer i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Is it wrong that the smell of hand sanitizer i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41156</th>\n",
       "      <td>44955</td>\n",
       "      <td>89907</td>\n",
       "      <td>i love you so much || he/him</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41157 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName                      Location     TweetAt  \\\n",
       "0          3799       48751                        London  16-03-2020   \n",
       "1          3800       48752                            UK  16-03-2020   \n",
       "2          3801       48753                     Vagabonds  16-03-2020   \n",
       "3          3802       48754                           NaN  16-03-2020   \n",
       "4          3803       48755                           NaN  16-03-2020   \n",
       "...         ...         ...                           ...         ...   \n",
       "41152     44951       89903  Wellington City, New Zealand  14-04-2020   \n",
       "41153     44952       89904                           NaN  14-04-2020   \n",
       "41154     44953       89905                           NaN  14-04-2020   \n",
       "41155     44954       89906                           NaN  14-04-2020   \n",
       "41156     44955       89907  i love you so much || he/him  14-04-2020   \n",
       "\n",
       "                                           OriginalTweet           Sentiment  \\\n",
       "0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1      advice Talk to your neighbours family to excha...            Positive   \n",
       "2      Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3      My food stock is not the only one which is emp...            Positive   \n",
       "4      Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "...                                                  ...                 ...   \n",
       "41152  Airline pilots offering to stock supermarket s...             Neutral   \n",
       "41153  Response to complaint not provided citing COVI...  Extremely Negative   \n",
       "41154  You know itÂs getting tough when @KameronWild...            Positive   \n",
       "41155  Is it wrong that the smell of hand sanitizer i...             Neutral   \n",
       "41156  @TartiiCat Well new/used Rift S are going for ...            Negative   \n",
       "\n",
       "                                          ProcessedTweet  \n",
       "0      @MeNyrbie @Phil_Gahan @Chrisitv url and url an...  \n",
       "1      advice Talk to your neighbours family to excha...  \n",
       "2      Coronavirus Australia: Woolworths to give elde...  \n",
       "3      My food stock is not the only one which is emp...  \n",
       "4      Me, ready to go at supermarket during the # CO...  \n",
       "...                                                  ...  \n",
       "41152  Airline pilots offering to stock supermarket s...  \n",
       "41153  Response to complaint not provided citing COVI...  \n",
       "41154  You know itÂs getting tough when @KameronWild...  \n",
       "41155  Is it wrong that the smell of hand sanitizer i...  \n",
       "41156  @TartiiCat Well new/used Rift S are going for ...  \n",
       "\n",
       "[41157 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "local-professor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Positive': 11422, 'Negative': 9917, 'Neutral': 7713, 'Extremely Positive': 6624, 'Extremely Negative': 5481})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(train_df[\"Sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "conceptual-extreme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral', 'Positive']\n",
      "{'Extremely Negative': 0, 'Extremely Positive': 1, 'Negative': 2, 'Neutral': 3, 'Positive': 4}\n",
      "{0: 'Extremely Negative', 1: 'Extremely Positive', 2: 'Negative', 3: 'Neutral', 4: 'Positive'}\n"
     ]
    }
   ],
   "source": [
    "clss_lst = sorted(list(set(Counter(train_df[\"Sentiment\"]))))\n",
    "clss_to_id = {k:i for i, k in enumerate(clss_lst)}\n",
    "id_to_clss = {i:k for i, k in enumerate(clss_lst)}\n",
    "\n",
    "print(clss_lst)\n",
    "print(clss_to_id)\n",
    "print(id_to_clss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-lunch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "liable-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"y\"] = train_df[\"Sentiment\"].apply(lambda x: clss_to_id[x])\n",
    "test_df[\"y\"]  = test_df[\"Sentiment\"].apply( lambda x: clss_to_id[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "split-patent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        3\n",
       "1        4\n",
       "2        4\n",
       "3        4\n",
       "4        0\n",
       "        ..\n",
       "41152    3\n",
       "41153    0\n",
       "41154    4\n",
       "41155    3\n",
       "41156    2\n",
       "Name: y, Length: 41157, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "legal-posting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, 4, 4, 4, 0, 4, 4, 3, 4, 2],\n",
       " [[0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 1],\n",
       "  [1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 1],\n",
       "  [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"y\"][:10].tolist(), np.array(pd.get_dummies(train_df[\"y\"]))[:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-basketball",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-active",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "toxic-anger",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data (41157, 8)   \n",
      "Test data  (3798, 8)\n"
     ]
    }
   ],
   "source": [
    "# get train and test data from the dataframe\n",
    "print(\"Train data {}   \\nTest data  {}\".format(train_df.shape, test_df.shape))\n",
    "train_x, train_y = train_df[\"ProcessedTweet\"], train_df[\"y\"]\n",
    "test_x,  test_y  = test_df[\"ProcessedTweet\"],  test_df[\"y\"]\n",
    "\n",
    "# create the dev set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, dev_x, train_y, dev_y = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, stratify=train_y)\n",
    "\n",
    "# sample a small part of the data : \n",
    "train_x, train_y = train_x[:1000], train_y[:1000]\n",
    "dev_x, dev_y     = dev_x[:1000],   dev_y[:1000]\n",
    "test_x, test_y   = test_x[:1000],  test_y[:1000]\n",
    "\n",
    "# train_y, dev_y, test_y = (np.array(pd.get_dummies(train_y)), \n",
    "#                           np.array(pd.get_dummies(dev_y)), \n",
    "#                           np.array(pd.get_dummies(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-sample",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "knowing-button",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from numpy.lib.function_base import append\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import copy, time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math \n",
    "\n",
    "\n",
    "from transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n",
    "from collections import Counter\n",
    "from datasets import list_metrics, load_metric\n",
    "import numpy as np\n",
    "import torch\n",
    "import random, os, copy\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "# from models.ds_cl_model import ClBertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sustained-talent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download configuration from huggingface.co and cache.\n",
    "config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ruled-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bright-activity",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create the tokenizer and the model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model_bl = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(id_to_clss.keys()))\n",
    "model_bl.cuda()\n",
    "model_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eleven-newton",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class textDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        assert(len(self.encodings[list(self.encodings.keys())[0]]) == len(self.labels))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx][:128]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "finished-nurse",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create the tokenized dataloader\n",
    "train_x_tok = model_tokenizer(train_x.tolist(), truncation=True, padding=True, max_length=128)\n",
    "dev_x_tok   = model_tokenizer(dev_x.tolist(),   truncation=True, padding=True, max_length=128)\n",
    "test_x_tok  = model_tokenizer(test_x.tolist(),  truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "boring-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(textDataset(train_x_tok, train_y.tolist()), shuffle=True, batch_size=32)\n",
    "dev_dl   = DataLoader(textDataset(dev_x_tok,   dev_y.tolist()),   shuffle=True, batch_size=32)\n",
    "test_dl  = DataLoader(textDataset(test_x_tok,  test_y.tolist()),  shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-husband",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-fifth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "discrete-bottom",
   "metadata": {
    "code_folding": [
     2,
     15,
     46
    ]
   },
   "outputs": [],
   "source": [
    "# create the train and evaluate fn\n",
    "def f1(y_true, y_pred, weighted=False, print_clfn_rpt=True):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    f1_macro    = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_micro    = f1_score(y_true, y_pred, average='micro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    if print_clfn_rpt:\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    print(\"accuracy score : \", accuracy_score(y_true, y_pred))\n",
    "    if weighted:\n",
    "        return f1_macro, f1_micro, f1_weighted\n",
    "    return f1_macro, f1_micro\n",
    "\n",
    "def evaluate_fn(model, eval_dataloader, name, device, return_lbls=False, return_probs=False):\n",
    "    model.eval()\n",
    "    tot_loss = []\n",
    "    y_tr, y_pr, y_probs = [], [], []\n",
    "    sftmx = torch.nn.Softmax(dim=1)\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        batch['labels'] = batch['labels'].type(torch.LongTensor).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        y_probs.extend(sftmx(logits).tolist())\n",
    "        y_pr.extend(predictions.tolist()); y_tr.extend(batch[\"labels\"].tolist())\n",
    "        tot_loss.append(loss.mean().item())\n",
    "    f1_macro, f1_micro, f1_weighted = f1(np.array(y_tr), np.array(y_pr), True)\n",
    "    print(\"Obtained a loss / f1 score of {:.3f} / {:.3f} on {} dataset.\".format(\n",
    "        np.mean(tot_loss), f1_weighted, name))\n",
    "    print(\"Obtained a f1 score {} macro / micro / weighted : {} / {} / {} \\n\\n\".format(\n",
    "        name, f1_macro, f1_micro, f1_weighted))\n",
    "    model.train()\n",
    "    print(\"********************************\")\n",
    "    \n",
    "    if return_lbls:\n",
    "        return y_pr\n",
    "    \n",
    "    if return_probs:\n",
    "        return y_probs\n",
    "    \n",
    "    return f1_weighted\n",
    "\n",
    "def train_fn(model, train_dataloader, eval_dataloader, device, n_epochs=5, lr=2e-5, \n",
    "             eval_per_epoch=4, load_best=True, weights=None, use_probab=False, eval_train=True):\n",
    "    # Create optimizer and the scheduler.\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n",
    "    num_training_steps = n_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "                                                   num_warmup_steps=num_training_steps//10, \n",
    "                                                   num_training_steps=num_training_steps)\n",
    "\n",
    "    # write the train loop\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    model.train(); eval_at = 1/eval_per_epoch;\n",
    "    best_f1 = 0.0\n",
    "    import random\n",
    "    import time\n",
    "    import uuid; \n",
    "    import os \n",
    "\n",
    "    aaaa = uuid.uuid4().hex.upper()\n",
    "    pth = \"./results/wts_{}\".format(aaaa)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"\\nTraining for epoch {} .....\".format(epoch))\n",
    "        next_eval_at = eval_at\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            # evaluate_fn(model, train_dataloader, \"train\", device); next_eval_at += eval_at\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels_old = copy.deepcopy(batch['labels'])\n",
    "            batch['labels'] = batch['labels'].type(torch.LongTensor).to(model.device)\n",
    "            wts = batch.get('wts', None)\n",
    "            if 'wts' in batch:\n",
    "                batch.pop('wts')\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            \n",
    "            loss.backward(); \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step(); lr_scheduler.step(); optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        eval_f1 = evaluate_fn(model, eval_dataloader, \" eval dl 1 \", device)\n",
    "        if eval_f1 > best_f1:\n",
    "            print(\"new best eval_f1 {}, saving the model ..... \".format(eval_f1))\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            model_to_save.save_pretrained(pth)\n",
    "            best_f1 = eval_f1\n",
    "\n",
    "    if load_best:\n",
    "        print(\"Loading best model with f1 score of .... {} .\".format(best_f1))\n",
    "        model_to_load = model.module if hasattr(model, \"module\") else model\n",
    "        model_to_load.from_pretrained(pth)\n",
    "        model.to(device)\n",
    "        os.system(\"rm -rf {}\".format(pth))\n",
    "        return model_to_load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "finished-boulder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ce35f4480044ea8dcf6febea0ced52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=160.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for epoch 0 .....\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       141\n",
      "           1       0.33      0.01      0.03       152\n",
      "           2       0.24      0.10      0.14       237\n",
      "           3       0.56      0.10      0.17       206\n",
      "           4       0.26      0.86      0.40       264\n",
      "\n",
      "   micro avg       0.27      0.27      0.27      1000\n",
      "   macro avg       0.28      0.21      0.15      1000\n",
      "weighted avg       0.29      0.27      0.18      1000\n",
      "\n",
      "accuracy score :  0.272\n",
      "Obtained a loss / f1 score of 1.571 / 0.178 on  eval dl 1  dataset.\n",
      "Obtained a f1 score  eval dl 1  macro / micro / weighted : 0.14720490506016556 / 0.272 / 0.1780195980655218 \n",
      "\n",
      "\n",
      "********************************\n",
      "new best eval_f1 0.1780195980655218, saving the model ..... \n",
      "\n",
      "Training for epoch 1 .....\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.11      0.16       141\n",
      "           1       0.37      0.12      0.19       152\n",
      "           2       0.28      0.05      0.09       237\n",
      "           3       0.58      0.12      0.20       206\n",
      "           4       0.27      0.83      0.41       264\n",
      "\n",
      "   micro avg       0.29      0.29      0.29      1000\n",
      "   macro avg       0.36      0.25      0.21      1000\n",
      "weighted avg       0.36      0.29      0.22      1000\n",
      "\n",
      "accuracy score :  0.291\n",
      "Obtained a loss / f1 score of 1.547 / 0.221 on  eval dl 1  dataset.\n",
      "Obtained a f1 score  eval dl 1  macro / micro / weighted : 0.20877812533149037 / 0.291 / 0.22137528549532584 \n",
      "\n",
      "\n",
      "********************************\n",
      "new best eval_f1 0.22137528549532584, saving the model ..... \n",
      "\n",
      "Training for epoch 2 .....\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.07      0.12       141\n",
      "           1       0.41      0.36      0.38       152\n",
      "           2       0.30      0.23      0.26       237\n",
      "           3       0.46      0.43      0.44       206\n",
      "           4       0.29      0.52      0.38       264\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      1000\n",
      "   macro avg       0.37      0.32      0.32      1000\n",
      "weighted avg       0.36      0.34      0.33      1000\n",
      "\n",
      "accuracy score :  0.344\n",
      "Obtained a loss / f1 score of 1.497 / 0.327 on  eval dl 1  dataset.\n",
      "Obtained a f1 score  eval dl 1  macro / micro / weighted : 0.31589759220110214 / 0.344 / 0.3266573730720072 \n",
      "\n",
      "\n",
      "********************************\n",
      "new best eval_f1 0.3266573730720072, saving the model ..... \n",
      "\n",
      "Training for epoch 3 .....\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.17      0.24       141\n",
      "           1       0.45      0.35      0.39       152\n",
      "           2       0.32      0.38      0.35       237\n",
      "           3       0.40      0.47      0.43       206\n",
      "           4       0.31      0.35      0.33       264\n",
      "\n",
      "   micro avg       0.36      0.36      0.36      1000\n",
      "   macro avg       0.38      0.35      0.35      1000\n",
      "weighted avg       0.37      0.36      0.35      1000\n",
      "\n",
      "accuracy score :  0.358\n",
      "Obtained a loss / f1 score of 1.449 / 0.353 on  eval dl 1  dataset.\n",
      "Obtained a f1 score  eval dl 1  macro / micro / weighted : 0.34988251745372595 / 0.35800000000000004 / 0.3531914287921777 \n",
      "\n",
      "\n",
      "********************************\n",
      "new best eval_f1 0.3531914287921777, saving the model ..... \n",
      "\n",
      "Training for epoch 4 .....\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.18      0.26       141\n",
      "           1       0.46      0.47      0.47       152\n",
      "           2       0.33      0.36      0.34       237\n",
      "           3       0.52      0.41      0.46       206\n",
      "           4       0.33      0.45      0.38       264\n",
      "\n",
      "   micro avg       0.39      0.39      0.39      1000\n",
      "   macro avg       0.41      0.37      0.38      1000\n",
      "weighted avg       0.40      0.39      0.38      1000\n",
      "\n",
      "accuracy score :  0.386\n",
      "Obtained a loss / f1 score of 1.442 / 0.383 on  eval dl 1  dataset.\n",
      "Obtained a f1 score  eval dl 1  macro / micro / weighted : 0.3809405959399109 / 0.386 / 0.383149896885469 \n",
      "\n",
      "\n",
      "********************************\n",
      "new best eval_f1 0.383149896885469, saving the model ..... \n",
      "Loading best model with f1 score of .... 0.383149896885469 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the model here;\n",
    "model_name = \"bert-base-uncased\"\n",
    "model_bl = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(id_to_clss.keys()))\n",
    "model_bl.cuda()\n",
    "model_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# call the train function on the dataloaders\n",
    "train_fn(model_bl, train_dl, dev_dl, model_bl.device, eval_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-appearance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "empty-possession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.16      0.22       146\n",
      "           1       0.41      0.29      0.34       162\n",
      "           2       0.36      0.25      0.30       302\n",
      "           3       0.36      0.45      0.40       152\n",
      "           4       0.26      0.45      0.33       238\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      1000\n",
      "   macro avg       0.34      0.32      0.32      1000\n",
      "weighted avg       0.34      0.32      0.32      1000\n",
      "\n",
      "accuracy score :  0.322\n",
      "Obtained a loss / f1 score of 1.507 / 0.315 on  test dl  dataset.\n",
      "Obtained a f1 score  test dl  macro / micro / weighted : 0.3168062679967473 / 0.322 / 0.31537022340409715 \n",
      "\n",
      "\n",
      "********************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31537022340409715"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_fn(model_bl, test_dl, \" test dl \", model_bl.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-parameter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-andrew",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-citation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-brief",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-highland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-hearing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-macintosh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-bouquet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-india",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-mountain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
