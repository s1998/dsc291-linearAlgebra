{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/spencersheen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Generate Data Matrix (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./Corona_NLP_train.csv\", \n",
    "                       encoding = \"ISO-8859-1\")\n",
    "test_df  = pd.read_csv(\"./Corona_NLP_test.csv\", \n",
    "                       encoding = \"ISO-8859-1\")\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def clean_string(ss):\n",
    "    ss = re.sub(r'https?://\\S+', 'url', ss)\n",
    "    ss = re.sub(r'#', ' # ', ss)\n",
    "    ss = re.sub(r'\\s+', ' ', ss)\n",
    "    ss = re.sub(r'[!\"\\$%&\\(\\)\\*\\+,\\.\\/:;<=>\\?@^_`{\\|}~]', '', ss)\n",
    "    \n",
    "    ss_new = \"\"\n",
    "    for word in ss.split(\" \"):\n",
    "        word = word.lower()\n",
    "        word = ps.stem(word)\n",
    "        if word not in stopwords_set:\n",
    "            ss_new = ss_new + \" \" + word\n",
    "    \n",
    "    return ss_new.strip()\n",
    "#     return ss\n",
    "\n",
    "train_df[\"ProcessedTweet\"] = train_df[\"OriginalTweet\"].apply(clean_string)\n",
    "test_df[\"ProcessedTweet\"]  = test_df[\"OriginalTweet\"].apply(clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coronaviru australia woolworth give elderli disabl dedic shop hour amid covid-19 outbreak url'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"ProcessedTweet\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54756"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "word_dict = {}\n",
    "for tweet in train_df[\"ProcessedTweet\"]:\n",
    "    for word in tweet.split(\" \"):\n",
    "        word = word.lower()\n",
    "        #stem\n",
    "        #remove stop words\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = count\n",
    "            count = count+1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((len(train_df[\"ProcessedTweet\"]), count))\n",
    "for i in range(0, len(train_df[\"ProcessedTweet\"])):\n",
    "    tweet_list = train_df[\"ProcessedTweet\"][i].split(\" \")\n",
    "    for j in range(0, len(tweet_list)):\n",
    "        word = tweet_list[j].lower()\n",
    "        if word in word_dict:\n",
    "            A[i][word_dict[word]] = A[i][word_dict[word]]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros(len(train_df[\"Sentiment\"]))\n",
    "for i in range(0, len(train_df[\"Sentiment\"])):\n",
    "    if train_df[\"Sentiment\"][i] == \"Extremely Negative\":\n",
    "        b[i] = 1\n",
    "    elif train_df[\"Sentiment\"][i] == \"Negative\":\n",
    "        b[i] = 2\n",
    "    elif train_df[\"Sentiment\"][i] == \"Neutral\":\n",
    "        b[i] = 3\n",
    "    elif train_df[\"Sentiment\"][i] == \"Positive\":\n",
    "        b[i] = 4\n",
    "    elif train_df[\"Sentiment\"][i] == \"Extremely Positive\":\n",
    "        b[i] = 5\n",
    "    else:\n",
    "        print(\"Something wrong happened\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Baseline (BOW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_sparse = sparse.csr_matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spencersheen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(A_sparse, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8014189566780864"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(A_sparse, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(A_sparse[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_test = np.zeros((len(test_df[\"ProcessedTweet\"]), count))\n",
    "for i in range(0, len(test_df[\"ProcessedTweet\"])):\n",
    "    tweet_list = test_df[\"ProcessedTweet\"][i].split(\" \")\n",
    "    for j in range(0, len(tweet_list)):\n",
    "        word = tweet_list[j].lower()\n",
    "        if word in word_dict:\n",
    "            A_test[i][word_dict[word]] = A_test[i][word_dict[word]]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_test = np.zeros(len(test_df[\"Sentiment\"]))\n",
    "for i in range(0, len(test_df[\"Sentiment\"])):\n",
    "    if test_df[\"Sentiment\"][i] == \"Extremely Negative\":\n",
    "        b_test[i] = 1\n",
    "    elif test_df[\"Sentiment\"][i] == \"Negative\":\n",
    "        b_test[i] = 2\n",
    "    elif test_df[\"Sentiment\"][i] == \"Neutral\":\n",
    "        b_test[i] = 3\n",
    "    elif test_df[\"Sentiment\"][i] == \"Positive\":\n",
    "        b_test[i] = 4\n",
    "    elif test_df[\"Sentiment\"][i] == \"Extremely Positive\":\n",
    "        b_test[i] = 5\n",
    "    else:\n",
    "        print(\"Something wrong happened\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_sparse_test = sparse.csr_matrix(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5903106898367562"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(A_sparse_test, b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro: 0.601081561824525\n",
      "micro: 0.5903106898367562\n",
      "weighted: 0.5898560616180559\n"
     ]
    }
   ],
   "source": [
    "b_pred = clf.predict(A_sparse_test)\n",
    "print(\"macro: \" + str(f1_score(b_test, b_pred, average='macro')))\n",
    "print(\"micro: \" + str(f1_score(b_test, b_pred, average='micro')))\n",
    "print(\"weighted: \" + str(f1_score(b_test, b_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 NNMF (BOW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=50, init='random', random_state=0)\n",
    "W = model.fit_transform(A_sparse)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/spencersheen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.9s finished\n"
     ]
    }
   ],
   "source": [
    "clf_nnmf = LogisticRegression(random_state=0, verbose=True).fit(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35857812765750663"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nnmf.score(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NMF(n_components=50, init='random', random_state=0)\n",
    "W = model.transform(A_sparse_test)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33754607688256977"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nnmf.score(W, b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro: 0.3036834091670501\n",
      "micro: 0.33754607688256977\n",
      "weighted: 0.31583480844771783\n"
     ]
    }
   ],
   "source": [
    "b_pred = clf_nnmf.predict(W)\n",
    "print(\"macro: \" + str(f1_score(b_test, b_pred, average='macro')))\n",
    "print(\"micro: \" + str(f1_score(b_test, b_pred, average='micro')))\n",
    "print(\"weighted: \" + str(f1_score(b_test, b_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 SVD (BOW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TruncatedSVD(n_components=50, random_state=0)\n",
    "A_SVD = model.fit_transform(A_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/spencersheen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "clf_svd = LogisticRegression(random_state=0, verbose=True).fit(A_SVD, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3847705129139636"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svd.score(A_SVD, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_SVD_test = model.transform(A_sparse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36545550289626116"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svd.score(A_SVD_test, b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro: 0.35887639395353\n",
      "micro: 0.36545550289626116\n",
      "weighted: 0.3567150814815877\n"
     ]
    }
   ],
   "source": [
    "b_pred = clf_svd.predict(A_SVD_test)\n",
    "print(\"macro: \" + str(f1_score(b_test, b_pred, average='macro')))\n",
    "print(\"micro: \" + str(f1_score(b_test, b_pred, average='micro')))\n",
    "print(\"weighted: \" + str(f1_score(b_test, b_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Creating Data Matrix (TFIDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 5000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(min_df=3,\n",
    "    max_df=0.85,\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(train_df[\"ProcessedTweet\"])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Baseline (TFIDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/spencersheen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.7s finished\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, verbose=True).fit(X, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7061739193818791"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_df[\"ProcessedTweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5468667719852554"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro: 0.5567301761861693\n",
      "micro: 0.5468667719852554\n",
      "weighted: 0.5474497438393306\n"
     ]
    }
   ],
   "source": [
    "b_pred = clf.predict(X_test)\n",
    "print(\"macro: \" + str(f1_score(b_test, b_pred, average='macro')))\n",
    "print(\"micro: \" + str(f1_score(b_test, b_pred, average='micro')))\n",
    "print(\"weighted: \" + str(f1_score(b_test, b_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 NNMF (TFIDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=50, init='random', random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simp = W.dot(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/spencersheen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   44.3s finished\n"
     ]
    }
   ],
   "source": [
    "clf_nnmf = LogisticRegression(random_state=0, verbose=True).fit(X_simp, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37784580994727507"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nnmf.score(X_simp, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_test = model.transform(X_test)\n",
    "H_test = model.components_\n",
    "X_simp_test = W_test.dot(H_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35018430753027907"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nnmf.score(X_simp_test, b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro: 0.3340356646014494\n",
      "micro: 0.35018430753027907\n",
      "weighted: 0.33995099242758303\n"
     ]
    }
   ],
   "source": [
    "b_pred = clf_nnmf.predict(X_simp_test)\n",
    "print(\"macro: \" + str(f1_score(b_test, b_pred, average='macro')))\n",
    "print(\"micro: \" + str(f1_score(b_test, b_pred, average='micro')))\n",
    "print(\"weighted: \" + str(f1_score(b_test, b_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 SVD (TFIDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TruncatedSVD(n_components=50, random_state=0)\n",
    "X_SVD = model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/spencersheen/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "clf_svd = LogisticRegression(random_state=0, verbose=True).fit(X_SVD, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3986442160507326"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svd.score(X_SVD, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SVD_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3762506582411796"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svd.score(X_SVD_test, b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro: 0.3698564115229537\n",
      "micro: 0.3762506582411796\n",
      "weighted: 0.3694722599588555\n"
     ]
    }
   ],
   "source": [
    "b_pred = clf_svd.predict(X_SVD_test)\n",
    "print(\"macro: \" + str(f1_score(b_test, b_pred, average='macro')))\n",
    "print(\"micro: \" + str(f1_score(b_test, b_pred, average='micro')))\n",
    "print(\"weighted: \" + str(f1_score(b_test, b_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
